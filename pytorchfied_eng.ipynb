{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words=pd.read_csv(\"data/amharic_names.csv\")\n",
    "old_words=open(\"data/cleaned_words.txt\",\"r\").read().splitlines()\n",
    "new_amh_words=new_words[\"in_am\"].tolist()\n",
    "am_words=new_amh_words + old_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs=sorted(set(list(\"\".join(am_words))))\n",
    "stoi={s:i+1 for i,s in enumerate(chrs)}\n",
    "stoi[\".\"]=0\n",
    "itos={i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=3\n",
    "def build_dataset(am_words):\n",
    "    X,Y=[],[]\n",
    "    for w in am_words:\n",
    "        context=[0]*block_size\n",
    "        for ch in w +\".\":\n",
    "            ix=stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context=context[1:]+[ix]\n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(am_words)\n",
    "n1=int(0.8*(len(am_words)))\n",
    "n2=int(0.9*(len(am_words)))\n",
    "\n",
    "Xtr,Ytr=build_dataset(am_words[:n1])      #80\n",
    "Xval,Yval=build_dataset(am_words[n1:n2])  #10\n",
    "Xte,Yte=build_dataset(am_words[n2:])      #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,fan_in,fan_out,bias=True):\n",
    "        self.weight=torch.randn((fan_in,fan_out),generator=g) / fan_in**0.5\n",
    "        self.bias=torch.zeros(fan_out) if bias else None \n",
    "    def __call__(self,x):\n",
    "        self.out=x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out +=self.bias\n",
    "        return self.out \n",
    "    def parameters(self):\n",
    "        return[self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self,dim,eps=1e-5,momentum=0.1):\n",
    "        self.eps=eps\n",
    "        self.momentum=momentum\n",
    "        self.training=True\n",
    "        #parameters(train with backprop)\n",
    "        self.gamma=torch.ones(dim)\n",
    "        self.beta=torch.zeros(dim)\n",
    "        #buffers(doesnt use grad trained with a running \"momentum update\")\n",
    "        self.running_mean=torch.zeros(dim)\n",
    "        self.running_var=torch.ones(dim)\n",
    "    def __call__(self,x):\n",
    "        if self.training:\n",
    "            xmean=x.mean(0,keepdim=True)\n",
    "            xvar=x.var(0,keepdim=True)\n",
    "        else:\n",
    "            xmean=self.running_mean\n",
    "            xvar=self.running_mean\n",
    "        xhat=(x-xmean)/torch.sqrt(xvar + self.eps) #normalization \n",
    "        self.out=self.gamma * xhat + self.beta\n",
    "        #update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean=(1-self.momentum)*self.running_mean + self.momentum * xmean\n",
    "                self.running_var= (1-self.momentum)*self.running_var  + self.momentum * xvar\n",
    "            return self.out       \n",
    "    def parameters(self):\n",
    "        return [self.gamma,self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self,x):\n",
    "        self.out=torch.tanh(x)\n",
    "        return self.out  \n",
    "    def parameters(self):\n",
    "        return []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed=10\n",
    "n_hidden=100\n",
    "block_size=3\n",
    "vocab_size=len(itos)\n",
    "g=torch.Generator().manual_seed(2147483647)\n",
    "C=torch.randn((vocab_size,n_embed),  generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-layers\n",
    "layers=[\n",
    "    Linear(n_embed*block_size, n_hidden),Tanh(),\n",
    "    Linear(          n_hidden, n_hidden),Tanh(),\n",
    "    Linear(          n_hidden, n_hidden),Tanh(),\n",
    "    Linear(          n_hidden, n_hidden),Tanh(),\n",
    "    Linear(          n_hidden, n_hidden),Tanh(),\n",
    "    Linear(          n_hidden, vocab_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66477\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    layers[-1].weight*=0.1 #last layer make less confident\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer,Linear):\n",
    "            layer.weight*=5/3   #other layers apply gain\n",
    "parameters=[C]+[p for layer in layers for p in layer.parameters()] \n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad=True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000   5.320193\n",
      "  10000/ 200000   1.916625\n",
      "  20000/ 200000   1.544427\n",
      "  30000/ 200000   1.540077\n",
      "  40000/ 200000   2.057544\n",
      "  50000/ 200000   1.603221\n",
      "  60000/ 200000   1.301067\n",
      "  70000/ 200000   1.272900\n",
      "  80000/ 200000   1.254606\n",
      "  90000/ 200000   1.386509\n",
      " 100000/ 200000   1.301937\n",
      " 110000/ 200000   1.096691\n",
      " 120000/ 200000   2.025812\n",
      " 130000/ 200000   1.207332\n",
      " 140000/ 200000   1.520185\n",
      " 150000/ 200000   0.991740\n",
      " 160000/ 200000   1.972781\n",
      " 170000/ 200000   1.916506\n",
      " 180000/ 200000   1.134367\n",
      " 190000/ 200000   1.227143\n"
     ]
    }
   ],
   "source": [
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix=torch.randint(0,Xtr.shape[0],(batch_size,),generator=g)\n",
    "    Xb,Yb=Xtr[ix],Ytr[ix]\n",
    "\n",
    "    embed=C[Xb]\n",
    "    x=embed.view(embed.shape[0],-1) # concat\n",
    "    for layer in layers:\n",
    "        x=layer(x)\n",
    "    loss=F.cross_entropy(x,Yb)\n",
    "\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "\n",
    "    lr=0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data+=-lr * p.grad\n",
    "\n",
    "    if i%10000==0:\n",
    "        print(f\"{i: 7d}/{max_steps: 6d}  {loss.item(): 4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
